{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "users = {}\n",
    "with open('data/users.csv', newline='') as csvfile:\n",
    "    # 讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "    users_info = csv.DictReader(csvfile)\n",
    "    for user in users_info:\n",
    "        users[user[\"user_id\"]] = {\n",
    "            \"gender\": user[\"gender\"],\n",
    "            \"occupation_titles\": user[\"occupation_titles\"],\n",
    "            \"interests\": user[\"interests\"]\n",
    "        }\n",
    "\n",
    "group2id = {}\n",
    "id2group = {}\n",
    "with open('data/subgroups.csv', newline='') as csvfile:\n",
    "    group = csv.DictReader(csvfile)\n",
    "    for i, subgroup in enumerate(group):\n",
    "        group2id[subgroup['subgroup_id']] = i\n",
    "        id2group[i] = subgroup['subgroup_id']\n",
    "len(group2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '5bdecbfffec014002166796a',\n",
       " 'interests': '職場技能_創業,設計_平面設計,藝術_電腦繪圖,藝術_繪畫與插畫,手作_刺繡,攝影_影像創作,手作_手作小物,服務業',\n",
       " 'subgroup': [26]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### train dataset\n",
    "train_dataset = []\n",
    "train_users_subgroup = {}   \n",
    "with open('data/train_group.csv', newline='') as csvfile:\n",
    "    train = csv.DictReader(csvfile)\n",
    "    for i in train:\n",
    "        if i[\"subgroup\"] == \"\":\n",
    "            train_users_subgroup[i[\"user_id\"]] = []\n",
    "            continue\n",
    "        user_train = {}\n",
    "        user_train[\"user_id\"] = i[\"user_id\"]\n",
    "        user_train[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        user_train[\"subgroup\"] = [group2id[subgroup] for subgroup in i[\"subgroup\"].split(\" \")]\n",
    "        train_users_subgroup[i[\"user_id\"]] = user_train[\"subgroup\"]\n",
    "        train_dataset.append(user_train)\n",
    "        \n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '612c1fcd560d8100069aa5ba',\n",
       " 'interests': '生活品味_寵物,手作_手作小物,生活品味_親子教育,手作_刺繡,生活品味_烹飪料理與甜點,金融業',\n",
       " 'subgroup': [7, 68, 69]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### validation dataset\n",
    "validation_dataset = []     \n",
    "with open('data/val_unseen_group.csv', newline='') as csvfile:\n",
    "    validation = csv.DictReader(csvfile)\n",
    "    for i in validation:\n",
    "        if i[\"subgroup\"] == \"\":\n",
    "            continue\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        user_validation[\"subgroup\"] = [group2id[subgroup] for subgroup in i[\"subgroup\"].split(\" \")]\n",
    "        validation_dataset.append(user_validation)\n",
    "        \n",
    "validation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenlin/anaconda3/envs/ADL/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_id', 'interests', 'subgroup'],\n",
       "    num_rows: 59032\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files={\"train\":\"new_data/train_group.csv\", \"validation\":\"new_data/val_seen_group.csv\"})\n",
    "# dataset[\"train\"][1]\n",
    "dataset = {}\n",
    "dataset['train'] = Dataset.from_list(train_dataset)\n",
    "dataset['validation'] = Dataset.from_list(validation_dataset)\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"interests\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels = np.zeros(len(group2id))\n",
    "  # fill numpy array\n",
    "  for id in examples[\"subgroup\"]:\n",
    "      labels[id] = 1\n",
    "\n",
    "  encoding[\"labels\"] = labels.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59032/59032 [00:18<00:00, 3155.02ex/s]\n",
      "100%|██████████| 11526/11526 [00:03<00:00, 3325.04ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 5480,\n",
       " 1842,\n",
       " 2825,\n",
       " 5543,\n",
       " 142,\n",
       " 1201,\n",
       " 3511,\n",
       " 117,\n",
       " 6257,\n",
       " 6243,\n",
       " 142,\n",
       " 2398,\n",
       " 7481,\n",
       " 6257,\n",
       " 6243,\n",
       " 117,\n",
       " 5971,\n",
       " 6123,\n",
       " 142,\n",
       " 7442,\n",
       " 5582,\n",
       " 5257,\n",
       " 1756,\n",
       " 117,\n",
       " 5971,\n",
       " 6123,\n",
       " 142,\n",
       " 5257,\n",
       " 4529,\n",
       " 5645,\n",
       " 2991,\n",
       " 4529,\n",
       " 117,\n",
       " 2797,\n",
       " 868,\n",
       " 142,\n",
       " 1173,\n",
       " 5255,\n",
       " 117,\n",
       " 3109,\n",
       " 2512,\n",
       " 142,\n",
       " 2512,\n",
       " 1008,\n",
       " 1201,\n",
       " 868,\n",
       " 117,\n",
       " 2797,\n",
       " 868,\n",
       " 142,\n",
       " 2797,\n",
       " 868,\n",
       " 2207,\n",
       " 4289,\n",
       " 117,\n",
       " 3302,\n",
       " 1243,\n",
       " 3511,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset = dataset['train'].map(preprocess_data, remove_columns=dataset['train'].column_names)\n",
    "encoded_validation_dataset = dataset['validation'].map(preprocess_data, remove_columns=dataset['validation'].column_names)\n",
    "example = encoded_train_dataset[0]\n",
    "# example = encoded_dataset['validation'][0]\n",
    "example[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset.set_format(\"torch\")\n",
    "encoded_validation_dataset.set_format(\"torch\")\n",
    "example['labels']\n",
    "#dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-bert-wwm-ext\", \n",
    "                                                           #problem_type=\"multi_label_classification\",\n",
    "                                                           #ignore_mismatched_sizes=True,\n",
    "                                                           num_labels=len(group2id))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 21:36:39.241465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-subgroup111\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "# def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "#     # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "#     sigmoid = torch.nn.Sigmoid()\n",
    "#     probs = sigmoid(torch.Tensor(predictions))\n",
    "#     # next, use threshold to turn them into integer predictions\n",
    "#     y_pred = np.zeros(probs.shape)\n",
    "#     y_pred[np.where(probs >= threshold)] = 1\n",
    "#     # finally, compute metrics\n",
    "#     y_true = labels\n",
    "#     f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "#     roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     # return as dictionary\n",
    "#     metrics = {'f1': f1_micro_average,\n",
    "#                'roc_auc': roc_auc,\n",
    "#                'accuracy': accuracy}\n",
    "#     return metrics\n",
    "\n",
    "def apk(actual, predicted, k=50):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "            \n",
    "    #print(score)\n",
    "    if len(actual) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=50):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return {'map@50' : np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])}\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    #print(preds, type(preds))\n",
    "    predicted = [np.argsort(pred)[::-1].tolist() for pred in preds]\n",
    "    actual = [np.where(label == 1)[0].tolist() for label in p.label_ids]\n",
    "\n",
    "    result = mapk(\n",
    "        actual=actual,\n",
    "        predicted=predicted)\n",
    "    return result\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenlin/anaconda3/envs/ADL/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 59032\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18450\n",
      "  0%|          | 0/18450 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model,\n\u001b[1;32m      3\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/ADL/lib/python3.8/site-packages/transformers/trainer.py:1521\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1518\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1519\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1520\u001b[0m )\n\u001b[0;32m-> 1521\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1522\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1523\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1524\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1525\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1526\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ADL/lib/python3.8/site-packages/transformers/trainer.py:1830\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1828\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[1;32m   1829\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1830\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1832\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   1833\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ADL/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:67\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     66\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ADL/lib/python3.8/site-packages/transformers/optimization.py:370\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    367\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    368\u001b[0m     step_size \u001b[39m=\u001b[39m step_size \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(bias_correction2) \u001b[39m/\u001b[39m bias_correction1\n\u001b[0;32m--> 370\u001b[0m p\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n\u001b[1;32m    372\u001b[0m \u001b[39m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[39m# Add weight decay at the end (fixed version)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test unseen dataset\n",
    "test_dataset = []     \n",
    "with open('data/test_unseen_group.csv', newline='') as csvfile:\n",
    "    test = csv.DictReader(csvfile)\n",
    "    for i in test:\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        test_dataset.append(user_validation)\n",
    "        \n",
    "dataset['test'] = Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11097/11097 [00:02<00:00, 3710.02ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  4415,\n",
       "  6512,\n",
       "  117,\n",
       "  4923,\n",
       "  2466,\n",
       "  142,\n",
       "  7030,\n",
       "  1265,\n",
       "  1146,\n",
       "  3358,\n",
       "  117,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  2832,\n",
       "  6536,\n",
       "  6223,\n",
       "  2573,\n",
       "  117,\n",
       "  5480,\n",
       "  1842,\n",
       "  2825,\n",
       "  5543,\n",
       "  142,\n",
       "  3126,\n",
       "  4372,\n",
       "  2990,\n",
       "  1285,\n",
       "  117,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  7032,\n",
       "  6084,\n",
       "  1555,\n",
       "  1501,\n",
       "  117,\n",
       "  5480,\n",
       "  1842,\n",
       "  2825,\n",
       "  5543,\n",
       "  142,\n",
       "  943,\n",
       "  782,\n",
       "  1501,\n",
       "  4277,\n",
       "  5195,\n",
       "  4245,\n",
       "  117,\n",
       "  6182,\n",
       "  6863,\n",
       "  3511,\n",
       "  117,\n",
       "  3302,\n",
       "  1243,\n",
       "  3511,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_test_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"interests\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  return encoding\n",
    "\n",
    "encoded_test_dataset = dataset['test'].map(preprocess_test_data, remove_columns=dataset['test'].column_names)\n",
    "encoded_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 11097\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[50,\n",
       " 58,\n",
       " 0,\n",
       " 63,\n",
       " 65,\n",
       " 6,\n",
       " 68,\n",
       " 36,\n",
       " 49,\n",
       " 28,\n",
       " 70,\n",
       " 71,\n",
       " 82,\n",
       " 60,\n",
       " 35,\n",
       " 74,\n",
       " 39,\n",
       " 75,\n",
       " 84,\n",
       " 56,\n",
       " 32,\n",
       " 34,\n",
       " 69,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 62,\n",
       " 22,\n",
       " 54,\n",
       " 14,\n",
       " 64,\n",
       " 59,\n",
       " 78,\n",
       " 52,\n",
       " 31,\n",
       " 80,\n",
       " 4,\n",
       " 18,\n",
       " 72,\n",
       " 55,\n",
       " 51,\n",
       " 83,\n",
       " 12,\n",
       " 3,\n",
       " 53,\n",
       " 23,\n",
       " 57,\n",
       " 33,\n",
       " 27,\n",
       " 67,\n",
       " 13,\n",
       " 61,\n",
       " 40,\n",
       " 38,\n",
       " 48,\n",
       " 77,\n",
       " 37,\n",
       " 86,\n",
       " 15,\n",
       " 29,\n",
       " 76,\n",
       " 20,\n",
       " 66,\n",
       " 46,\n",
       " 24,\n",
       " 79,\n",
       " 41,\n",
       " 44,\n",
       " 42,\n",
       " 45,\n",
       " 11,\n",
       " 8,\n",
       " 47,\n",
       " 89,\n",
       " 90,\n",
       " 88,\n",
       " 73,\n",
       " 87,\n",
       " 21,\n",
       " 25,\n",
       " 1,\n",
       " 19,\n",
       " 26,\n",
       " 43,\n",
       " 81,\n",
       " 9,\n",
       " 30,\n",
       " 10,\n",
       " 85,\n",
       " 17,\n",
       " 16]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_dataset.set_format(\"torch\")\n",
    "predicts = trainer.predict(encoded_test_dataset)\n",
    "predicts_list = [np.argsort(pred)[::-1].tolist() for pred in predicts.predictions]\n",
    "predicts_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen subgroup done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('predict_outputs/bertchinese_unseen_subgroup111.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['user_id', 'subgroup'])\n",
    "    \n",
    "    for i, predicts in enumerate(predicts_list):\n",
    "        subgroup = \" \".join([str(id2group[pred]) for pred in predicts])\n",
    "        writer.writerow([test_dataset[i][\"user_id\"], subgroup])\n",
    "print(\"unseen subgroup done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7205/7205 [00:02<00:00, 3393.09ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  4495,\n",
       "  3833,\n",
       "  1501,\n",
       "  1456,\n",
       "  142,\n",
       "  2187,\n",
       "  4289,\n",
       "  117,\n",
       "  6121,\n",
       "  7077,\n",
       "  142,\n",
       "  3152,\n",
       "  3428,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  2398,\n",
       "  7481,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  1240,\n",
       "  2706,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  3109,\n",
       "  2512,\n",
       "  142,\n",
       "  1555,\n",
       "  3511,\n",
       "  3109,\n",
       "  2512,\n",
       "  117,\n",
       "  3109,\n",
       "  2512,\n",
       "  142,\n",
       "  2527,\n",
       "  6182,\n",
       "  1198,\n",
       "  6744,\n",
       "  117,\n",
       "  6121,\n",
       "  7077,\n",
       "  142,\n",
       "  3149,\n",
       "  855,\n",
       "  6121,\n",
       "  7077,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  5206,\n",
       "  7514,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  5971,\n",
       "  3152,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  2451,\n",
       "  1440,\n",
       "  1001,\n",
       "  3064,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### test seen dataset\n",
    "test_dataset = []     \n",
    "with open('data/test_seen_group.csv', newline='') as csvfile:\n",
    "    test = csv.DictReader(csvfile)\n",
    "    for i in test:\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        test_dataset.append(user_validation)\n",
    "        \n",
    "dataset['test'] = Dataset.from_list(test_dataset)\n",
    "encoded_test_dataset = dataset['test'].map(preprocess_test_data, remove_columns=dataset['test'].column_names)\n",
    "encoded_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 7205\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 65,\n",
       " 50,\n",
       " 71,\n",
       " 49,\n",
       " 39,\n",
       " 6,\n",
       " 5,\n",
       " 22,\n",
       " 58,\n",
       " 60,\n",
       " 70,\n",
       " 0,\n",
       " 4,\n",
       " 35,\n",
       " 28,\n",
       " 18,\n",
       " 52,\n",
       " 33,\n",
       " 72,\n",
       " 38,\n",
       " 37,\n",
       " 36,\n",
       " 64,\n",
       " 82,\n",
       " 77,\n",
       " 24,\n",
       " 3,\n",
       " 84,\n",
       " 76,\n",
       " 59,\n",
       " 32,\n",
       " 7,\n",
       " 68,\n",
       " 78,\n",
       " 54,\n",
       " 66,\n",
       " 55,\n",
       " 34,\n",
       " 53,\n",
       " 12,\n",
       " 69,\n",
       " 31,\n",
       " 63,\n",
       " 56,\n",
       " 14,\n",
       " 61,\n",
       " 8,\n",
       " 21,\n",
       " 23,\n",
       " 40,\n",
       " 13,\n",
       " 27,\n",
       " 48,\n",
       " 89,\n",
       " 29,\n",
       " 26,\n",
       " 86,\n",
       " 75,\n",
       " 74,\n",
       " 20,\n",
       " 45,\n",
       " 44,\n",
       " 62,\n",
       " 80,\n",
       " 15,\n",
       " 19,\n",
       " 51,\n",
       " 11,\n",
       " 83,\n",
       " 57,\n",
       " 73,\n",
       " 88,\n",
       " 46,\n",
       " 90,\n",
       " 25,\n",
       " 85,\n",
       " 1,\n",
       " 47,\n",
       " 87,\n",
       " 42,\n",
       " 43,\n",
       " 41,\n",
       " 67,\n",
       " 79,\n",
       " 16,\n",
       " 10,\n",
       " 9,\n",
       " 17,\n",
       " 81,\n",
       " 30]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_dataset.set_format(\"torch\")\n",
    "predicts = trainer.predict(encoded_test_dataset)\n",
    "predicts_list = [np.argsort(pred)[::-1].tolist() for pred in predicts.predictions]\n",
    "predicts_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen subgroup done\n"
     ]
    }
   ],
   "source": [
    "with open('predict_outputs/bertchinese_seen_group111.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['user_id', 'subgroup'])\n",
    "    \n",
    "    for i, predicts in enumerate(predicts_list):\n",
    "        subgroup = \" \".join([str(id2group[pred]) for pred in predicts if pred not in train_users_subgroup[test_dataset[i][\"user_id\"]]])\n",
    "        writer.writerow([test_dataset[i][\"user_id\"], subgroup])\n",
    "print(\"seen subgroup done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c11ab1cd7ffeebfddd49b3f1bd3d90b59ad807fa9f67b05cb040d95cb976482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
