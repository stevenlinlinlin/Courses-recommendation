{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "users = {}\n",
    "with open('data/users.csv', newline='') as csvfile:\n",
    "    # 讀取 CSV 檔內容，將每一列轉成一個 dictionary\n",
    "    users_info = csv.DictReader(csvfile)\n",
    "    for user in users_info:\n",
    "        users[user[\"user_id\"]] = {\n",
    "            \"gender\": user[\"gender\"],\n",
    "            \"occupation_titles\": user[\"occupation_titles\"],\n",
    "            \"interests\": user[\"interests\"]\n",
    "        }\n",
    "\n",
    "course2id = {}\n",
    "id2course = {}\n",
    "with open('data/courses.csv', newline='') as csvfile:\n",
    "    courses = csv.DictReader(csvfile)\n",
    "    for i, course in enumerate(courses):\n",
    "        course2id[course['course_id']] = i\n",
    "        id2course[i] = course['course_id']\n",
    "len(course2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 295]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### train dataset\n",
    "train_dataset = []\n",
    "train_users_course = {}     \n",
    "with open('data/train.csv', newline='') as csvfile:\n",
    "    train = csv.DictReader(csvfile)\n",
    "    for i in train:\n",
    "        user_train = {}\n",
    "        user_train[\"user_id\"] = i[\"user_id\"]\n",
    "        user_train[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        user_train[\"course_id\"] = [course2id[course] for course in i[\"course_id\"].split(\" \")]\n",
    "        train_users_course[i[\"user_id\"]] = user_train[\"course_id\"]\n",
    "        train_dataset.append(user_train)\n",
    "        \n",
    "#train_dataset[0]\n",
    "train_users_course[i[\"user_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '56dae2b74e3ef90900b7bd0e',\n",
       " 'interests': '程式_程式入門,程式_資料科學,職場技能_求職,語言_英文,程式_程式語言,資訊科技',\n",
       " 'course_id': [247]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### validation dataset\n",
    "validation_dataset = []     \n",
    "with open('data/val_seen.csv', newline='') as csvfile:\n",
    "    validation = csv.DictReader(csvfile)\n",
    "    for i in validation:\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        user_validation[\"course_id\"] = [course2id[course] for course in i[\"course_id\"].split(\" \")]\n",
    "        validation_dataset.append(user_validation)\n",
    "        \n",
    "validation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenlin/anaconda3/envs/ADL/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['user_id', 'interests', 'course_id'],\n",
       "    num_rows: 59737\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"csv\", data_files={\"train\":\"new_data/train_group.csv\", \"validation\":\"new_data/val_seen_group.csv\"})\n",
    "# dataset[\"train\"][1]\n",
    "dataset = {}\n",
    "dataset['train'] = Dataset.from_list(train_dataset)\n",
    "dataset['validation'] = Dataset.from_list(validation_dataset)\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"interests\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  # create numpy array of shape (batch_size, num_labels)\n",
    "  labels = np.zeros(len(course2id))\n",
    "  # fill numpy array\n",
    "  for id in examples[\"course_id\"]:\n",
    "      labels[id] = 1\n",
    "\n",
    "  encoding[\"labels\"] = labels.tolist()\n",
    "  \n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59737/59737 [00:20<00:00, 2878.11ex/s]\n",
      "100%|██████████| 7748/7748 [00:02<00:00, 2713.46ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 5480,\n",
       " 1842,\n",
       " 2825,\n",
       " 5543,\n",
       " 142,\n",
       " 1201,\n",
       " 3511,\n",
       " 117,\n",
       " 6257,\n",
       " 6243,\n",
       " 142,\n",
       " 2398,\n",
       " 7481,\n",
       " 6257,\n",
       " 6243,\n",
       " 117,\n",
       " 5971,\n",
       " 6123,\n",
       " 142,\n",
       " 7442,\n",
       " 5582,\n",
       " 5257,\n",
       " 1756,\n",
       " 117,\n",
       " 5971,\n",
       " 6123,\n",
       " 142,\n",
       " 5257,\n",
       " 4529,\n",
       " 5645,\n",
       " 2991,\n",
       " 4529,\n",
       " 117,\n",
       " 2797,\n",
       " 868,\n",
       " 142,\n",
       " 1173,\n",
       " 5255,\n",
       " 117,\n",
       " 3109,\n",
       " 2512,\n",
       " 142,\n",
       " 2512,\n",
       " 1008,\n",
       " 1201,\n",
       " 868,\n",
       " 117,\n",
       " 2797,\n",
       " 868,\n",
       " 142,\n",
       " 2797,\n",
       " 868,\n",
       " 2207,\n",
       " 4289,\n",
       " 117,\n",
       " 3302,\n",
       " 1243,\n",
       " 3511,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset = dataset['train'].map(preprocess_data, remove_columns=dataset['train'].column_names)\n",
    "encoded_validation_dataset = dataset['validation'].map(preprocess_data, remove_columns=dataset['validation'].column_names)\n",
    "example = encoded_train_dataset[0]\n",
    "# example = encoded_dataset['validation'][0]\n",
    "example[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_dataset.set_format(\"torch\")\n",
    "encoded_validation_dataset.set_format(\"torch\")\n",
    "example['labels']\n",
    "#dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-bert-wwm-ext\", \n",
    "                                                           #problem_type=\"multi_label_classification\",\n",
    "                                                           #ignore_mismatched_sizes=True,\n",
    "                                                           num_labels=len(course2id))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-29 21:37:17.918688: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-course0\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    #load_best_model_at_end=True,\n",
    "    #metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "# def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "#     # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "#     sigmoid = torch.nn.Sigmoid()\n",
    "#     probs = sigmoid(torch.Tensor(predictions))\n",
    "#     # next, use threshold to turn them into integer predictions\n",
    "#     y_pred = np.zeros(probs.shape)\n",
    "#     y_pred[np.where(probs >= threshold)] = 1\n",
    "#     # finally, compute metrics\n",
    "#     y_true = labels\n",
    "#     f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "#     roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "#     accuracy = accuracy_score(y_true, y_pred)\n",
    "#     # return as dictionary\n",
    "#     metrics = {'f1': f1_micro_average,\n",
    "#                'roc_auc': roc_auc,\n",
    "#                'accuracy': accuracy}\n",
    "#     return metrics\n",
    "\n",
    "def apk(actual, predicted, k=50):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "            \n",
    "    #print(score)\n",
    "    if len(actual) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=50):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted \n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return {'map@50' : np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])}\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    #print(preds, type(preds))\n",
    "    predicted = [np.argsort(pred)[::-1].tolist() for pred in preds]\n",
    "    actual = [np.where(label == 1)[0].tolist() for label in p.label_ids]\n",
    "\n",
    "    result = mapk(\n",
    "        actual=actual,\n",
    "        predicted=predicted)\n",
    "    return result\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevenlin/anaconda3/envs/ADL/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 59737\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18670\n",
      "  0%|          | 0/18670 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  3%|▎         | 501/18670 [01:11<43:58,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1572, 'learning_rate': 1.9464381360471345e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1001/18670 [02:24<43:30,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0273, 'learning_rate': 1.892876272094269e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1501/18670 [03:38<42:53,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0204, 'learning_rate': 1.8393144081414034e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2001/18670 [04:53<41:20,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0185, 'learning_rate': 1.785752544188538e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2501/18670 [06:08<40:37,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0178, 'learning_rate': 1.7321906802356724e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3001/18670 [07:23<39:21,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0175, 'learning_rate': 1.6786288162828067e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3501/18670 [08:38<38:34,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'learning_rate': 1.6250669523299413e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 3733/18670 [09:13<36:38,  6.79it/s]***** Running Evaluation *****\n",
      "  Num examples = 7748\n",
      "  Batch size = 16\n",
      "                                                    \n",
      " 20%|██        | 3734/18670 [09:36<36:38,  6.79it/s]Saving model checkpoint to bert-finetuned-course0/checkpoint-3734\n",
      "Configuration saved in bert-finetuned-course0/checkpoint-3734/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021309418603777885, 'eval_map@50': 0.06162341986375953, 'eval_runtime': 23.6594, 'eval_samples_per_second': 327.481, 'eval_steps_per_second': 20.499, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-course0/checkpoint-3734/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-course0/checkpoint-3734/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-course0/checkpoint-3734/special_tokens_map.json\n",
      " 21%|██▏       | 4001/18670 [10:31<37:24,  6.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0174, 'learning_rate': 1.5715050883770756e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 4501/18670 [11:45<35:30,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'learning_rate': 1.5179432244242101e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 5001/18670 [13:00<34:17,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0169, 'learning_rate': 1.4643813604713446e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5501/18670 [14:15<33:09,  6.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0171, 'learning_rate': 1.4108194965184789e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6001/18670 [15:30<31:49,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'learning_rate': 1.3572576325656135e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 6501/18670 [16:45<30:32,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0171, 'learning_rate': 1.3036957686127478e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7001/18670 [18:00<29:29,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'learning_rate': 1.2501339046598821e-05, 'epoch': 1.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 7467/18670 [19:10<27:46,  6.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 7748\n",
      "  Batch size = 16\n",
      "                                                    \n",
      " 40%|████      | 7468/18670 [19:34<27:45,  6.72it/s]Saving model checkpoint to bert-finetuned-course0/checkpoint-7468\n",
      "Configuration saved in bert-finetuned-course0/checkpoint-7468/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.022877763956785202, 'eval_map@50': 0.05899716669401654, 'eval_runtime': 24.1572, 'eval_samples_per_second': 320.732, 'eval_steps_per_second': 20.077, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-course0/checkpoint-7468/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-course0/checkpoint-7468/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-course0/checkpoint-7468/special_tokens_map.json\n",
      " 40%|████      | 7501/18670 [19:53<28:11,  6.60it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'learning_rate': 1.1965720407070168e-05, 'epoch': 2.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 8001/18670 [21:08<26:19,  6.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0172, 'learning_rate': 1.1430101767541511e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 8501/18670 [22:22<25:09,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0169, 'learning_rate': 1.0894483128012856e-05, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 9001/18670 [23:36<24:07,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 1.0358864488484199e-05, 'epoch': 2.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 9501/18670 [24:51<22:58,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0169, 'learning_rate': 9.823245848955545e-06, 'epoch': 2.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 10001/18670 [26:06<21:51,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 9.28762720942689e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 10501/18670 [27:21<20:31,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0169, 'learning_rate': 8.752008569898233e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 11001/18670 [28:37<19:15,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 8.216389930369578e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 11201/18670 [29:07<18:49,  6.61it/s]***** Running Evaluation *****\n",
      "  Num examples = 7748\n",
      "  Batch size = 16\n",
      "                                                     \n",
      " 60%|██████    | 11202/18670 [29:31<18:49,  6.61it/s]Saving model checkpoint to bert-finetuned-course0/checkpoint-11202\n",
      "Configuration saved in bert-finetuned-course0/checkpoint-11202/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.024334268644452095, 'eval_map@50': 0.0619876636643889, 'eval_runtime': 24.0724, 'eval_samples_per_second': 321.863, 'eval_steps_per_second': 20.148, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-course0/checkpoint-11202/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-course0/checkpoint-11202/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-course0/checkpoint-11202/special_tokens_map.json\n",
      " 62%|██████▏   | 11501/18670 [30:30<17:49,  6.70it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.017, 'learning_rate': 7.68077129084092e-06, 'epoch': 3.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 12001/18670 [31:45<16:45,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0167, 'learning_rate': 7.145152651312266e-06, 'epoch': 3.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 12501/18670 [33:00<15:20,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 6.60953401178361e-06, 'epoch': 3.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 13001/18670 [34:14<14:24,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0167, 'learning_rate': 6.073915372254955e-06, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13501/18670 [35:29<12:55,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 5.538296732726299e-06, 'epoch': 3.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 14001/18670 [36:44<11:49,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 5.002678093197644e-06, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 14501/18670 [38:00<10:30,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0168, 'learning_rate': 4.4670594536689885e-06, 'epoch': 3.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 14935/18670 [39:05<09:19,  6.68it/s]***** Running Evaluation *****\n",
      "  Num examples = 7748\n",
      "  Batch size = 16\n",
      "                                                     \n",
      " 80%|████████  | 14936/18670 [39:30<09:19,  6.68it/s]Saving model checkpoint to bert-finetuned-course0/checkpoint-14936\n",
      "Configuration saved in bert-finetuned-course0/checkpoint-14936/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02544930763542652, 'eval_map@50': 0.06723627386326213, 'eval_runtime': 24.2448, 'eval_samples_per_second': 319.574, 'eval_steps_per_second': 20.004, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-course0/checkpoint-14936/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-course0/checkpoint-14936/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-course0/checkpoint-14936/special_tokens_map.json\n",
      " 80%|████████  | 15001/18670 [39:54<09:12,  6.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 3.931440814140332e-06, 'epoch': 4.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 15501/18670 [41:09<08:03,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0164, 'learning_rate': 3.3958221746116767e-06, 'epoch': 4.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 16001/18670 [42:24<06:39,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 2.860203535083021e-06, 'epoch': 4.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 16501/18670 [43:39<05:25,  6.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0165, 'learning_rate': 2.3245848955543654e-06, 'epoch': 4.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 17001/18670 [44:53<04:11,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0163, 'learning_rate': 1.78896625602571e-06, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 17501/18670 [46:09<02:56,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 1.2533476164970543e-06, 'epoch': 4.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 18001/18670 [47:23<01:40,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 7.177289769683986e-07, 'epoch': 4.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 18501/18670 [48:38<00:25,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0166, 'learning_rate': 1.821103374397429e-07, 'epoch': 4.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 18669/18670 [49:03<00:00,  6.73it/s]***** Running Evaluation *****\n",
      "  Num examples = 7748\n",
      "  Batch size = 16\n",
      "                                                     \n",
      "100%|██████████| 18670/18670 [49:27<00:00,  6.73it/s]Saving model checkpoint to bert-finetuned-course0/checkpoint-18670\n",
      "Configuration saved in bert-finetuned-course0/checkpoint-18670/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026051407679915428, 'eval_map@50': 0.06778748196609237, 'eval_runtime': 24.1123, 'eval_samples_per_second': 321.33, 'eval_steps_per_second': 20.114, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in bert-finetuned-course0/checkpoint-18670/pytorch_model.bin\n",
      "tokenizer config file saved in bert-finetuned-course0/checkpoint-18670/tokenizer_config.json\n",
      "Special tokens file saved in bert-finetuned-course0/checkpoint-18670/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 18670/18670 [49:35<00:00,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2975.9515, 'train_samples_per_second': 100.366, 'train_steps_per_second': 6.274, 'train_loss': 0.021052546799981024, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18670, training_loss=0.021052546799981024, metrics={'train_runtime': 2975.9515, 'train_samples_per_second': 100.366, 'train_steps_per_second': 6.274, 'train_loss': 0.021052546799981024, 'epoch': 5.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test unseen dataset\n",
    "test_dataset = []     \n",
    "with open('data/test_unseen.csv', newline='') as csvfile:\n",
    "    test = csv.DictReader(csvfile)\n",
    "    for i in test:\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        test_dataset.append(user_validation)\n",
    "        \n",
    "dataset['test'] = Dataset.from_list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11097/11097 [00:03<00:00, 3665.29ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  4415,\n",
       "  6512,\n",
       "  117,\n",
       "  4923,\n",
       "  2466,\n",
       "  142,\n",
       "  7030,\n",
       "  1265,\n",
       "  1146,\n",
       "  3358,\n",
       "  117,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  2832,\n",
       "  6536,\n",
       "  6223,\n",
       "  2573,\n",
       "  117,\n",
       "  5480,\n",
       "  1842,\n",
       "  2825,\n",
       "  5543,\n",
       "  142,\n",
       "  3126,\n",
       "  4372,\n",
       "  2990,\n",
       "  1285,\n",
       "  117,\n",
       "  2832,\n",
       "  6536,\n",
       "  4415,\n",
       "  6512,\n",
       "  142,\n",
       "  7032,\n",
       "  6084,\n",
       "  1555,\n",
       "  1501,\n",
       "  117,\n",
       "  5480,\n",
       "  1842,\n",
       "  2825,\n",
       "  5543,\n",
       "  142,\n",
       "  943,\n",
       "  782,\n",
       "  1501,\n",
       "  4277,\n",
       "  5195,\n",
       "  4245,\n",
       "  117,\n",
       "  6182,\n",
       "  6863,\n",
       "  3511,\n",
       "  117,\n",
       "  3302,\n",
       "  1243,\n",
       "  3511,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_test_data(examples):\n",
    "  # take a batch of texts\n",
    "  text = examples[\"interests\"]\n",
    "  # encode them\n",
    "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "  return encoding\n",
    "\n",
    "encoded_test_dataset = dataset['test'].map(preprocess_test_data, remove_columns=dataset['test'].column_names)\n",
    "encoded_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 11097\n",
      "  Batch size = 16\n",
      "100%|██████████| 694/694 [00:32<00:00, 21.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[533,\n",
       " 560,\n",
       " 603,\n",
       " 599,\n",
       " 500,\n",
       " 502,\n",
       " 501,\n",
       " 425,\n",
       " 644,\n",
       " 569,\n",
       " 631,\n",
       " 496,\n",
       " 563,\n",
       " 426,\n",
       " 573,\n",
       " 391,\n",
       " 557,\n",
       " 620,\n",
       " 586,\n",
       " 424,\n",
       " 270,\n",
       " 314,\n",
       " 531,\n",
       " 185,\n",
       " 530,\n",
       " 624,\n",
       " 288,\n",
       " 613,\n",
       " 416,\n",
       " 462,\n",
       " 591,\n",
       " 590,\n",
       " 70,\n",
       " 509,\n",
       " 226,\n",
       " 625,\n",
       " 638,\n",
       " 524,\n",
       " 616,\n",
       " 287,\n",
       " 53,\n",
       " 202,\n",
       " 589,\n",
       " 652,\n",
       " 418,\n",
       " 295,\n",
       " 516,\n",
       " 164,\n",
       " 595,\n",
       " 598,\n",
       " 653,\n",
       " 112,\n",
       " 182,\n",
       " 126,\n",
       " 430,\n",
       " 522,\n",
       " 159,\n",
       " 581,\n",
       " 614,\n",
       " 632,\n",
       " 580,\n",
       " 610,\n",
       " 145,\n",
       " 593,\n",
       " 514,\n",
       " 374,\n",
       " 300,\n",
       " 579,\n",
       " 385,\n",
       " 654,\n",
       " 341,\n",
       " 477,\n",
       " 605,\n",
       " 645,\n",
       " 618,\n",
       " 130,\n",
       " 512,\n",
       " 310,\n",
       " 627,\n",
       " 567,\n",
       " 403,\n",
       " 626,\n",
       " 575,\n",
       " 92,\n",
       " 124,\n",
       " 594,\n",
       " 291,\n",
       " 493,\n",
       " 592,\n",
       " 503,\n",
       " 177,\n",
       " 231,\n",
       " 67,\n",
       " 643,\n",
       " 64,\n",
       " 639,\n",
       " 191,\n",
       " 583,\n",
       " 161,\n",
       " 572,\n",
       " 157,\n",
       " 648,\n",
       " 378,\n",
       " 611,\n",
       " 621,\n",
       " 332,\n",
       " 454,\n",
       " 206,\n",
       " 132,\n",
       " 84,\n",
       " 228,\n",
       " 218,\n",
       " 343,\n",
       " 281,\n",
       " 401,\n",
       " 122,\n",
       " 552,\n",
       " 264,\n",
       " 87,\n",
       " 100,\n",
       " 308,\n",
       " 559,\n",
       " 255,\n",
       " 634,\n",
       " 117,\n",
       " 246,\n",
       " 604,\n",
       " 506,\n",
       " 376,\n",
       " 353,\n",
       " 622,\n",
       " 597,\n",
       " 362,\n",
       " 241,\n",
       " 602,\n",
       " 609,\n",
       " 155,\n",
       " 617,\n",
       " 446,\n",
       " 189,\n",
       " 372,\n",
       " 585,\n",
       " 196,\n",
       " 174,\n",
       " 550,\n",
       " 115,\n",
       " 148,\n",
       " 555,\n",
       " 55,\n",
       " 405,\n",
       " 66,\n",
       " 267,\n",
       " 10,\n",
       " 494,\n",
       " 574,\n",
       " 420,\n",
       " 394,\n",
       " 293,\n",
       " 27,\n",
       " 600,\n",
       " 34,\n",
       " 452,\n",
       " 472,\n",
       " 259,\n",
       " 492,\n",
       " 548,\n",
       " 465,\n",
       " 354,\n",
       " 423,\n",
       " 473,\n",
       " 154,\n",
       " 427,\n",
       " 266,\n",
       " 584,\n",
       " 570,\n",
       " 131,\n",
       " 141,\n",
       " 242,\n",
       " 315,\n",
       " 547,\n",
       " 299,\n",
       " 476,\n",
       " 78,\n",
       " 402,\n",
       " 40,\n",
       " 351,\n",
       " 409,\n",
       " 587,\n",
       " 152,\n",
       " 338,\n",
       " 217,\n",
       " 642,\n",
       " 219,\n",
       " 321,\n",
       " 526,\n",
       " 165,\n",
       " 565,\n",
       " 525,\n",
       " 419,\n",
       " 322,\n",
       " 357,\n",
       " 670,\n",
       " 540,\n",
       " 651,\n",
       " 636,\n",
       " 456,\n",
       " 623,\n",
       " 150,\n",
       " 635,\n",
       " 335,\n",
       " 187,\n",
       " 497,\n",
       " 208,\n",
       " 486,\n",
       " 537,\n",
       " 307,\n",
       " 640,\n",
       " 216,\n",
       " 74,\n",
       " 225,\n",
       " 396,\n",
       " 451,\n",
       " 342,\n",
       " 630,\n",
       " 596,\n",
       " 646,\n",
       " 387,\n",
       " 510,\n",
       " 369,\n",
       " 271,\n",
       " 539,\n",
       " 566,\n",
       " 641,\n",
       " 577,\n",
       " 289,\n",
       " 209,\n",
       " 120,\n",
       " 237,\n",
       " 490,\n",
       " 344,\n",
       " 633,\n",
       " 239,\n",
       " 326,\n",
       " 62,\n",
       " 223,\n",
       " 257,\n",
       " 107,\n",
       " 508,\n",
       " 520,\n",
       " 647,\n",
       " 306,\n",
       " 663,\n",
       " 52,\n",
       " 42,\n",
       " 345,\n",
       " 379,\n",
       " 562,\n",
       " 466,\n",
       " 380,\n",
       " 108,\n",
       " 457,\n",
       " 381,\n",
       " 532,\n",
       " 485,\n",
       " 672,\n",
       " 629,\n",
       " 28,\n",
       " 258,\n",
       " 316,\n",
       " 32,\n",
       " 38,\n",
       " 564,\n",
       " 197,\n",
       " 542,\n",
       " 619,\n",
       " 505,\n",
       " 470,\n",
       " 521,\n",
       " 123,\n",
       " 608,\n",
       " 220,\n",
       " 31,\n",
       " 232,\n",
       " 657,\n",
       " 263,\n",
       " 398,\n",
       " 50,\n",
       " 415,\n",
       " 331,\n",
       " 33,\n",
       " 543,\n",
       " 195,\n",
       " 297,\n",
       " 527,\n",
       " 142,\n",
       " 519,\n",
       " 102,\n",
       " 302,\n",
       " 386,\n",
       " 230,\n",
       " 439,\n",
       " 551,\n",
       " 170,\n",
       " 203,\n",
       " 178,\n",
       " 365,\n",
       " 181,\n",
       " 588,\n",
       " 628,\n",
       " 489,\n",
       " 247,\n",
       " 72,\n",
       " 176,\n",
       " 73,\n",
       " 227,\n",
       " 1,\n",
       " 601,\n",
       " 571,\n",
       " 346,\n",
       " 414,\n",
       " 248,\n",
       " 286,\n",
       " 276,\n",
       " 333,\n",
       " 61,\n",
       " 180,\n",
       " 659,\n",
       " 190,\n",
       " 128,\n",
       " 69,\n",
       " 668,\n",
       " 660,\n",
       " 469,\n",
       " 615,\n",
       " 612,\n",
       " 254,\n",
       " 240,\n",
       " 186,\n",
       " 160,\n",
       " 495,\n",
       " 568,\n",
       " 317,\n",
       " 89,\n",
       " 460,\n",
       " 245,\n",
       " 546,\n",
       " 194,\n",
       " 46,\n",
       " 373,\n",
       " 184,\n",
       " 25,\n",
       " 3,\n",
       " 234,\n",
       " 545,\n",
       " 319,\n",
       " 214,\n",
       " 275,\n",
       " 429,\n",
       " 554,\n",
       " 121,\n",
       " 453,\n",
       " 339,\n",
       " 43,\n",
       " 669,\n",
       " 549,\n",
       " 436,\n",
       " 382,\n",
       " 98,\n",
       " 359,\n",
       " 172,\n",
       " 296,\n",
       " 499,\n",
       " 116,\n",
       " 118,\n",
       " 468,\n",
       " 163,\n",
       " 578,\n",
       " 192,\n",
       " 479,\n",
       " 280,\n",
       " 661,\n",
       " 26,\n",
       " 328,\n",
       " 65,\n",
       " 97,\n",
       " 205,\n",
       " 360,\n",
       " 606,\n",
       " 637,\n",
       " 523,\n",
       " 252,\n",
       " 662,\n",
       " 607,\n",
       " 262,\n",
       " 91,\n",
       " 658,\n",
       " 168,\n",
       " 15,\n",
       " 238,\n",
       " 507,\n",
       " 4,\n",
       " 278,\n",
       " 114,\n",
       " 179,\n",
       " 464,\n",
       " 576,\n",
       " 105,\n",
       " 445,\n",
       " 535,\n",
       " 455,\n",
       " 407,\n",
       " 204,\n",
       " 397,\n",
       " 153,\n",
       " 213,\n",
       " 313,\n",
       " 444,\n",
       " 364,\n",
       " 370,\n",
       " 513,\n",
       " 199,\n",
       " 325,\n",
       " 125,\n",
       " 44,\n",
       " 88,\n",
       " 140,\n",
       " 561,\n",
       " 366,\n",
       " 81,\n",
       " 487,\n",
       " 541,\n",
       " 48,\n",
       " 272,\n",
       " 413,\n",
       " 284,\n",
       " 169,\n",
       " 478,\n",
       " 480,\n",
       " 371,\n",
       " 111,\n",
       " 399,\n",
       " 41,\n",
       " 193,\n",
       " 86,\n",
       " 274,\n",
       " 488,\n",
       " 139,\n",
       " 212,\n",
       " 536,\n",
       " 649,\n",
       " 553,\n",
       " 29,\n",
       " 301,\n",
       " 76,\n",
       " 558,\n",
       " 54,\n",
       " 292,\n",
       " 19,\n",
       " 173,\n",
       " 515,\n",
       " 243,\n",
       " 215,\n",
       " 475,\n",
       " 298,\n",
       " 21,\n",
       " 481,\n",
       " 188,\n",
       " 504,\n",
       " 146,\n",
       " 324,\n",
       " 68,\n",
       " 484,\n",
       " 129,\n",
       " 511,\n",
       " 35,\n",
       " 134,\n",
       " 556,\n",
       " 106,\n",
       " 483,\n",
       " 162,\n",
       " 304,\n",
       " 47,\n",
       " 406,\n",
       " 323,\n",
       " 261,\n",
       " 6,\n",
       " 211,\n",
       " 198,\n",
       " 158,\n",
       " 356,\n",
       " 253,\n",
       " 467,\n",
       " 544,\n",
       " 390,\n",
       " 14,\n",
       " 395,\n",
       " 411,\n",
       " 233,\n",
       " 85,\n",
       " 383,\n",
       " 459,\n",
       " 363,\n",
       " 20,\n",
       " 518,\n",
       " 156,\n",
       " 60,\n",
       " 458,\n",
       " 8,\n",
       " 368,\n",
       " 447,\n",
       " 222,\n",
       " 294,\n",
       " 207,\n",
       " 431,\n",
       " 352,\n",
       " 144,\n",
       " 337,\n",
       " 110,\n",
       " 23,\n",
       " 273,\n",
       " 437,\n",
       " 422,\n",
       " 94,\n",
       " 375,\n",
       " 99,\n",
       " 57,\n",
       " 135,\n",
       " 538,\n",
       " 221,\n",
       " 244,\n",
       " 432,\n",
       " 101,\n",
       " 269,\n",
       " 358,\n",
       " 171,\n",
       " 210,\n",
       " 340,\n",
       " 183,\n",
       " 421,\n",
       " 471,\n",
       " 277,\n",
       " 408,\n",
       " 36,\n",
       " 12,\n",
       " 167,\n",
       " 96,\n",
       " 113,\n",
       " 24,\n",
       " 350,\n",
       " 290,\n",
       " 440,\n",
       " 200,\n",
       " 80,\n",
       " 312,\n",
       " 51,\n",
       " 138,\n",
       " 7,\n",
       " 392,\n",
       " 329,\n",
       " 265,\n",
       " 534,\n",
       " 388,\n",
       " 282,\n",
       " 318,\n",
       " 309,\n",
       " 491,\n",
       " 59,\n",
       " 75,\n",
       " 283,\n",
       " 143,\n",
       " 377,\n",
       " 268,\n",
       " 260,\n",
       " 327,\n",
       " 442,\n",
       " 95,\n",
       " 393,\n",
       " 529,\n",
       " 330,\n",
       " 582,\n",
       " 2,\n",
       " 655,\n",
       " 443,\n",
       " 671,\n",
       " 666,\n",
       " 249,\n",
       " 448,\n",
       " 355,\n",
       " 438,\n",
       " 367,\n",
       " 384,\n",
       " 235,\n",
       " 39,\n",
       " 229,\n",
       " 11,\n",
       " 400,\n",
       " 127,\n",
       " 79,\n",
       " 433,\n",
       " 71,\n",
       " 5,\n",
       " 311,\n",
       " 82,\n",
       " 412,\n",
       " 224,\n",
       " 428,\n",
       " 251,\n",
       " 348,\n",
       " 49,\n",
       " 482,\n",
       " 104,\n",
       " 103,\n",
       " 45,\n",
       " 63,\n",
       " 201,\n",
       " 320,\n",
       " 279,\n",
       " 498,\n",
       " 133,\n",
       " 450,\n",
       " 417,\n",
       " 441,\n",
       " 136,\n",
       " 336,\n",
       " 137,\n",
       " 303,\n",
       " 22,\n",
       " 77,\n",
       " 236,\n",
       " 434,\n",
       " 256,\n",
       " 361,\n",
       " 151,\n",
       " 404,\n",
       " 305,\n",
       " 528,\n",
       " 93,\n",
       " 449,\n",
       " 474,\n",
       " 175,\n",
       " 461,\n",
       " 285,\n",
       " 166,\n",
       " 347,\n",
       " 517,\n",
       " 9,\n",
       " 463,\n",
       " 149,\n",
       " 109,\n",
       " 410,\n",
       " 349,\n",
       " 250,\n",
       " 675,\n",
       " 56,\n",
       " 147,\n",
       " 674,\n",
       " 90,\n",
       " 334,\n",
       " 13,\n",
       " 83,\n",
       " 30,\n",
       " 16,\n",
       " 17,\n",
       " 389,\n",
       " 679,\n",
       " 716,\n",
       " 709,\n",
       " 702,\n",
       " 715,\n",
       " 676,\n",
       " 683,\n",
       " 720,\n",
       " 700,\n",
       " 656,\n",
       " 726,\n",
       " 710,\n",
       " 725,\n",
       " 677,\n",
       " 692,\n",
       " 696,\n",
       " 691,\n",
       " 693,\n",
       " 687,\n",
       " 711,\n",
       " 707,\n",
       " 685,\n",
       " 680,\n",
       " 723,\n",
       " 722,\n",
       " 706,\n",
       " 665,\n",
       " 667,\n",
       " 719,\n",
       " 724,\n",
       " 58,\n",
       " 708,\n",
       " 717,\n",
       " 650,\n",
       " 694,\n",
       " 690,\n",
       " 721,\n",
       " 705,\n",
       " 119,\n",
       " 697,\n",
       " 37,\n",
       " 435,\n",
       " 686,\n",
       " 689,\n",
       " 701,\n",
       " 704,\n",
       " 682,\n",
       " 0,\n",
       " 684,\n",
       " 695,\n",
       " 698,\n",
       " 699,\n",
       " 703,\n",
       " 714,\n",
       " 678,\n",
       " 681,\n",
       " 673,\n",
       " 718,\n",
       " 688,\n",
       " 727,\n",
       " 713,\n",
       " 712,\n",
       " 664,\n",
       " 18]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_dataset.set_format(\"torch\")\n",
    "predicts = trainer.predict(encoded_test_dataset)\n",
    "predicts_list = [np.argsort(pred)[::-1].tolist() for pred in predicts.predictions]\n",
    "predicts_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unseen course done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('predict_outputs/bertchinese_unseen_course999.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['user_id', 'course_id'])\n",
    "    \n",
    "    for i, predicts in enumerate(predicts_list):\n",
    "        courses = \" \".join([str(id2course[pred]) for pred in predicts])\n",
    "        writer.writerow([test_dataset[i][\"user_id\"], courses])\n",
    "print(\"unseen course done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7205/7205 [00:02<00:00, 3306.44ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  4495,\n",
       "  3833,\n",
       "  1501,\n",
       "  1456,\n",
       "  142,\n",
       "  2187,\n",
       "  4289,\n",
       "  117,\n",
       "  6121,\n",
       "  7077,\n",
       "  142,\n",
       "  3152,\n",
       "  3428,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  2398,\n",
       "  7481,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  1240,\n",
       "  2706,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  3109,\n",
       "  2512,\n",
       "  142,\n",
       "  1555,\n",
       "  3511,\n",
       "  3109,\n",
       "  2512,\n",
       "  117,\n",
       "  3109,\n",
       "  2512,\n",
       "  142,\n",
       "  2527,\n",
       "  6182,\n",
       "  1198,\n",
       "  6744,\n",
       "  117,\n",
       "  6121,\n",
       "  7077,\n",
       "  142,\n",
       "  3149,\n",
       "  855,\n",
       "  6121,\n",
       "  7077,\n",
       "  117,\n",
       "  6257,\n",
       "  6243,\n",
       "  142,\n",
       "  5206,\n",
       "  7514,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  5971,\n",
       "  3152,\n",
       "  6257,\n",
       "  6243,\n",
       "  117,\n",
       "  2451,\n",
       "  1440,\n",
       "  1001,\n",
       "  3064,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### test seen dataset\n",
    "test_dataset = []     \n",
    "with open('data/test_seen.csv', newline='') as csvfile:\n",
    "    test = csv.DictReader(csvfile)\n",
    "    for i in test:\n",
    "        user_validation = {}\n",
    "        user_validation[\"user_id\"] = i[\"user_id\"]\n",
    "        user_validation[\"interests\"] = users[i[\"user_id\"]][\"interests\"] + ',' +users[i[\"user_id\"]][\"occupation_titles\"] if users[i[\"user_id\"]][\"occupation_titles\"] != \"\" else users[i[\"user_id\"]][\"interests\"]\n",
    "        test_dataset.append(user_validation)\n",
    "        \n",
    "dataset['test'] = Dataset.from_list(test_dataset)\n",
    "encoded_test_dataset = dataset['test'].map(preprocess_test_data, remove_columns=dataset['test'].column_names)\n",
    "encoded_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 7205\n",
      "  Batch size = 16\n",
      "100%|██████████| 451/451 [00:21<00:00, 21.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[501,\n",
       " 533,\n",
       " 500,\n",
       " 560,\n",
       " 425,\n",
       " 502,\n",
       " 603,\n",
       " 426,\n",
       " 524,\n",
       " 424,\n",
       " 573,\n",
       " 599,\n",
       " 616,\n",
       " 652,\n",
       " 531,\n",
       " 226,\n",
       " 624,\n",
       " 563,\n",
       " 530,\n",
       " 391,\n",
       " 631,\n",
       " 614,\n",
       " 416,\n",
       " 126,\n",
       " 70,\n",
       " 644,\n",
       " 627,\n",
       " 593,\n",
       " 569,\n",
       " 557,\n",
       " 496,\n",
       " 159,\n",
       " 314,\n",
       " 124,\n",
       " 586,\n",
       " 287,\n",
       " 620,\n",
       " 288,\n",
       " 132,\n",
       " 270,\n",
       " 185,\n",
       " 589,\n",
       " 613,\n",
       " 430,\n",
       " 590,\n",
       " 122,\n",
       " 206,\n",
       " 509,\n",
       " 610,\n",
       " 182,\n",
       " 653,\n",
       " 591,\n",
       " 632,\n",
       " 164,\n",
       " 462,\n",
       " 155,\n",
       " 625,\n",
       " 580,\n",
       " 174,\n",
       " 202,\n",
       " 385,\n",
       " 506,\n",
       " 595,\n",
       " 514,\n",
       " 522,\n",
       " 191,\n",
       " 654,\n",
       " 92,\n",
       " 100,\n",
       " 477,\n",
       " 638,\n",
       " 454,\n",
       " 112,\n",
       " 341,\n",
       " 516,\n",
       " 53,\n",
       " 567,\n",
       " 300,\n",
       " 246,\n",
       " 148,\n",
       " 295,\n",
       " 228,\n",
       " 418,\n",
       " 581,\n",
       " 255,\n",
       " 67,\n",
       " 622,\n",
       " 626,\n",
       " 634,\n",
       " 597,\n",
       " 177,\n",
       " 598,\n",
       " 648,\n",
       " 575,\n",
       " 291,\n",
       " 592,\n",
       " 145,\n",
       " 611,\n",
       " 84,\n",
       " 583,\n",
       " 493,\n",
       " 267,\n",
       " 645,\n",
       " 579,\n",
       " 310,\n",
       " 423,\n",
       " 403,\n",
       " 594,\n",
       " 332,\n",
       " 643,\n",
       " 419,\n",
       " 130,\n",
       " 55,\n",
       " 66,\n",
       " 621,\n",
       " 605,\n",
       " 639,\n",
       " 150,\n",
       " 512,\n",
       " 374,\n",
       " 117,\n",
       " 351,\n",
       " 465,\n",
       " 141,\n",
       " 473,\n",
       " 494,\n",
       " 362,\n",
       " 394,\n",
       " 231,\n",
       " 572,\n",
       " 457,\n",
       " 602,\n",
       " 630,\n",
       " 189,\n",
       " 258,\n",
       " 31,\n",
       " 372,\n",
       " 618,\n",
       " 161,\n",
       " 452,\n",
       " 405,\n",
       " 402,\n",
       " 401,\n",
       " 574,\n",
       " 472,\n",
       " 552,\n",
       " 378,\n",
       " 585,\n",
       " 196,\n",
       " 635,\n",
       " 559,\n",
       " 562,\n",
       " 490,\n",
       " 587,\n",
       " 335,\n",
       " 241,\n",
       " 219,\n",
       " 115,\n",
       " 64,\n",
       " 338,\n",
       " 343,\n",
       " 308,\n",
       " 154,\n",
       " 547,\n",
       " 570,\n",
       " 353,\n",
       " 187,\n",
       " 600,\n",
       " 315,\n",
       " 629,\n",
       " 216,\n",
       " 208,\n",
       " 108,\n",
       " 420,\n",
       " 131,\n",
       " 218,\n",
       " 525,\n",
       " 52,\n",
       " 157,\n",
       " 120,\n",
       " 609,\n",
       " 266,\n",
       " 396,\n",
       " 497,\n",
       " 476,\n",
       " 427,\n",
       " 503,\n",
       " 321,\n",
       " 281,\n",
       " 526,\n",
       " 379,\n",
       " 264,\n",
       " 299,\n",
       " 550,\n",
       " 237,\n",
       " 42,\n",
       " 123,\n",
       " 549,\n",
       " 555,\n",
       " 642,\n",
       " 446,\n",
       " 74,\n",
       " 345,\n",
       " 197,\n",
       " 271,\n",
       " 152,\n",
       " 259,\n",
       " 387,\n",
       " 657,\n",
       " 492,\n",
       " 34,\n",
       " 601,\n",
       " 203,\n",
       " 548,\n",
       " 565,\n",
       " 519,\n",
       " 486,\n",
       " 293,\n",
       " 409,\n",
       " 451,\n",
       " 40,\n",
       " 376,\n",
       " 604,\n",
       " 87,\n",
       " 566,\n",
       " 540,\n",
       " 651,\n",
       " 78,\n",
       " 217,\n",
       " 354,\n",
       " 577,\n",
       " 223,\n",
       " 307,\n",
       " 527,\n",
       " 564,\n",
       " 306,\n",
       " 209,\n",
       " 623,\n",
       " 640,\n",
       " 326,\n",
       " 633,\n",
       " 257,\n",
       " 456,\n",
       " 160,\n",
       " 663,\n",
       " 10,\n",
       " 672,\n",
       " 380,\n",
       " 195,\n",
       " 316,\n",
       " 227,\n",
       " 466,\n",
       " 32,\n",
       " 369,\n",
       " 532,\n",
       " 596,\n",
       " 539,\n",
       " 263,\n",
       " 239,\n",
       " 247,\n",
       " 225,\n",
       " 537,\n",
       " 606,\n",
       " 520,\n",
       " 381,\n",
       " 646,\n",
       " 62,\n",
       " 415,\n",
       " 636,\n",
       " 470,\n",
       " 584,\n",
       " 242,\n",
       " 73,\n",
       " 659,\n",
       " 360,\n",
       " 342,\n",
       " 27,\n",
       " 386,\n",
       " 38,\n",
       " 554,\n",
       " 414,\n",
       " 617,\n",
       " 184,\n",
       " 142,\n",
       " 647,\n",
       " 72,\n",
       " 165,\n",
       " 232,\n",
       " 296,\n",
       " 286,\n",
       " 612,\n",
       " 357,\n",
       " 365,\n",
       " 608,\n",
       " 607,\n",
       " 317,\n",
       " 254,\n",
       " 505,\n",
       " 176,\n",
       " 668,\n",
       " 50,\n",
       " 543,\n",
       " 220,\n",
       " 373,\n",
       " 107,\n",
       " 33,\n",
       " 186,\n",
       " 661,\n",
       " 61,\n",
       " 230,\n",
       " 170,\n",
       " 116,\n",
       " 485,\n",
       " 359,\n",
       " 628,\n",
       " 289,\n",
       " 1,\n",
       " 205,\n",
       " 619,\n",
       " 561,\n",
       " 571,\n",
       " 3,\n",
       " 339,\n",
       " 346,\n",
       " 333,\n",
       " 489,\n",
       " 641,\n",
       " 284,\n",
       " 508,\n",
       " 670,\n",
       " 344,\n",
       " 542,\n",
       " 28,\n",
       " 114,\n",
       " 128,\n",
       " 398,\n",
       " 214,\n",
       " 521,\n",
       " 487,\n",
       " 510,\n",
       " 41,\n",
       " 453,\n",
       " 331,\n",
       " 213,\n",
       " 178,\n",
       " 212,\n",
       " 199,\n",
       " 495,\n",
       " 568,\n",
       " 48,\n",
       " 660,\n",
       " 240,\n",
       " 234,\n",
       " 245,\n",
       " 181,\n",
       " 172,\n",
       " 276,\n",
       " 89,\n",
       " 382,\n",
       " 444,\n",
       " 637,\n",
       " 46,\n",
       " 252,\n",
       " 588,\n",
       " 535,\n",
       " 102,\n",
       " 469,\n",
       " 25,\n",
       " 546,\n",
       " 248,\n",
       " 88,\n",
       " 551,\n",
       " 460,\n",
       " 322,\n",
       " 455,\n",
       " 192,\n",
       " 319,\n",
       " 649,\n",
       " 168,\n",
       " 278,\n",
       " 429,\n",
       " 26,\n",
       " 35,\n",
       " 578,\n",
       " 153,\n",
       " 297,\n",
       " 121,\n",
       " 169,\n",
       " 86,\n",
       " 129,\n",
       " 507,\n",
       " 98,\n",
       " 662,\n",
       " 615,\n",
       " 43,\n",
       " 499,\n",
       " 304,\n",
       " 371,\n",
       " 397,\n",
       " 21,\n",
       " 576,\n",
       " 69,\n",
       " 364,\n",
       " 302,\n",
       " 65,\n",
       " 94,\n",
       " 356,\n",
       " 445,\n",
       " 118,\n",
       " 545,\n",
       " 4,\n",
       " 280,\n",
       " 29,\n",
       " 144,\n",
       " 479,\n",
       " 97,\n",
       " 180,\n",
       " 483,\n",
       " 464,\n",
       " 468,\n",
       " 313,\n",
       " 173,\n",
       " 658,\n",
       " 193,\n",
       " 111,\n",
       " 407,\n",
       " 105,\n",
       " 47,\n",
       " 298,\n",
       " 215,\n",
       " 504,\n",
       " 439,\n",
       " 325,\n",
       " 163,\n",
       " 15,\n",
       " 190,\n",
       " 337,\n",
       " 179,\n",
       " 467,\n",
       " 399,\n",
       " 458,\n",
       " 488,\n",
       " 44,\n",
       " 558,\n",
       " 436,\n",
       " 204,\n",
       " 188,\n",
       " 238,\n",
       " 54,\n",
       " 669,\n",
       " 366,\n",
       " 194,\n",
       " 513,\n",
       " 125,\n",
       " 76,\n",
       " 556,\n",
       " 19,\n",
       " 523,\n",
       " 68,\n",
       " 292,\n",
       " 139,\n",
       " 328,\n",
       " 275,\n",
       " 211,\n",
       " 106,\n",
       " 81,\n",
       " 162,\n",
       " 408,\n",
       " 8,\n",
       " 390,\n",
       " 80,\n",
       " 422,\n",
       " 484,\n",
       " 370,\n",
       " 324,\n",
       " 447,\n",
       " 283,\n",
       " 475,\n",
       " 243,\n",
       " 143,\n",
       " 511,\n",
       " 210,\n",
       " 383,\n",
       " 432,\n",
       " 553,\n",
       " 536,\n",
       " 269,\n",
       " 358,\n",
       " 480,\n",
       " 12,\n",
       " 158,\n",
       " 541,\n",
       " 368,\n",
       " 6,\n",
       " 459,\n",
       " 91,\n",
       " 352,\n",
       " 323,\n",
       " 221,\n",
       " 134,\n",
       " 363,\n",
       " 57,\n",
       " 518,\n",
       " 200,\n",
       " 272,\n",
       " 406,\n",
       " 481,\n",
       " 431,\n",
       " 183,\n",
       " 253,\n",
       " 75,\n",
       " 301,\n",
       " 274,\n",
       " 113,\n",
       " 20,\n",
       " 146,\n",
       " 478,\n",
       " 440,\n",
       " 262,\n",
       " 443,\n",
       " 96,\n",
       " 85,\n",
       " 261,\n",
       " 23,\n",
       " 393,\n",
       " 544,\n",
       " 207,\n",
       " 244,\n",
       " 350,\n",
       " 99,\n",
       " 156,\n",
       " 515,\n",
       " 110,\n",
       " 59,\n",
       " 367,\n",
       " 437,\n",
       " 14,\n",
       " 140,\n",
       " 198,\n",
       " 294,\n",
       " 413,\n",
       " 355,\n",
       " 7,\n",
       " 442,\n",
       " 233,\n",
       " 36,\n",
       " 340,\n",
       " 329,\n",
       " 138,\n",
       " 51,\n",
       " 392,\n",
       " 229,\n",
       " 273,\n",
       " 411,\n",
       " 388,\n",
       " 24,\n",
       " 529,\n",
       " 377,\n",
       " 222,\n",
       " 265,\n",
       " 318,\n",
       " 167,\n",
       " 327,\n",
       " 2,\n",
       " 79,\n",
       " 448,\n",
       " 666,\n",
       " 11,\n",
       " 95,\n",
       " 655,\n",
       " 60,\n",
       " 268,\n",
       " 538,\n",
       " 395,\n",
       " 282,\n",
       " 433,\n",
       " 471,\n",
       " 77,\n",
       " 303,\n",
       " 171,\n",
       " 421,\n",
       " 236,\n",
       " 49,\n",
       " 136,\n",
       " 101,\n",
       " 135,\n",
       " 312,\n",
       " 277,\n",
       " 400,\n",
       " 330,\n",
       " 534,\n",
       " 224,\n",
       " 428,\n",
       " 45,\n",
       " 375,\n",
       " 309,\n",
       " 63,\n",
       " 582,\n",
       " 127,\n",
       " 491,\n",
       " 348,\n",
       " 290,\n",
       " 256,\n",
       " 482,\n",
       " 82,\n",
       " 235,\n",
       " 311,\n",
       " 71,\n",
       " 438,\n",
       " 384,\n",
       " 260,\n",
       " 39,\n",
       " 249,\n",
       " 133,\n",
       " 441,\n",
       " 103,\n",
       " 417,\n",
       " 279,\n",
       " 104,\n",
       " 137,\n",
       " 336,\n",
       " 450,\n",
       " 251,\n",
       " 528,\n",
       " 361,\n",
       " 434,\n",
       " 9,\n",
       " 498,\n",
       " 412,\n",
       " 151,\n",
       " 201,\n",
       " 320,\n",
       " 671,\n",
       " 5,\n",
       " 22,\n",
       " 347,\n",
       " 305,\n",
       " 474,\n",
       " 449,\n",
       " 463,\n",
       " 404,\n",
       " 90,\n",
       " 166,\n",
       " 93,\n",
       " 285,\n",
       " 149,\n",
       " 349,\n",
       " 675,\n",
       " 461,\n",
       " 250,\n",
       " 674,\n",
       " 109,\n",
       " 56,\n",
       " 517,\n",
       " 175,\n",
       " 147,\n",
       " 334,\n",
       " 83,\n",
       " 410,\n",
       " 17,\n",
       " 13,\n",
       " 30,\n",
       " 389,\n",
       " 16,\n",
       " 685,\n",
       " 665,\n",
       " 715,\n",
       " 716,\n",
       " 706,\n",
       " 693,\n",
       " 700,\n",
       " 119,\n",
       " 723,\n",
       " 721,\n",
       " 678,\n",
       " 726,\n",
       " 711,\n",
       " 709,\n",
       " 719,\n",
       " 686,\n",
       " 696,\n",
       " 704,\n",
       " 697,\n",
       " 681,\n",
       " 679,\n",
       " 698,\n",
       " 690,\n",
       " 725,\n",
       " 689,\n",
       " 713,\n",
       " 717,\n",
       " 708,\n",
       " 37,\n",
       " 702,\n",
       " 676,\n",
       " 710,\n",
       " 724,\n",
       " 684,\n",
       " 718,\n",
       " 682,\n",
       " 656,\n",
       " 667,\n",
       " 687,\n",
       " 435,\n",
       " 714,\n",
       " 705,\n",
       " 701,\n",
       " 720,\n",
       " 0,\n",
       " 707,\n",
       " 695,\n",
       " 703,\n",
       " 688,\n",
       " 727,\n",
       " 712,\n",
       " 691,\n",
       " 694,\n",
       " 650,\n",
       " 664,\n",
       " 722,\n",
       " 680,\n",
       " 673,\n",
       " 692,\n",
       " 683,\n",
       " 18,\n",
       " 58,\n",
       " 677,\n",
       " 699]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_test_dataset.set_format(\"torch\")\n",
    "predicts = trainer.predict(encoded_test_dataset)\n",
    "predicts_list = [np.argsort(pred)[::-1].tolist() for pred in predicts.predictions]\n",
    "predicts_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen course done\n"
     ]
    }
   ],
   "source": [
    "with open('predict_outputs/bertchinese_seen_course999.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['user_id', 'course_id'])\n",
    "    \n",
    "    for i, predicts in enumerate(predicts_list):\n",
    "        courses = \" \".join([str(id2course[pred]) for pred in predicts if pred not in train_users_course[test_dataset[i][\"user_id\"]]])\n",
    "        #print(len(courses))\n",
    "        writer.writerow([test_dataset[i][\"user_id\"], courses])\n",
    "print(\"seen course done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7c11ab1cd7ffeebfddd49b3f1bd3d90b59ad807fa9f67b05cb040d95cb976482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
